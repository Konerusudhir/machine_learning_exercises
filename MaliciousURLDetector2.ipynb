{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Konerusudhir/machine_learning_exercises/blob/master/MaliciousURLDetector2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Malicious URL Detection\n",
        "Dataset for this problem is from [Kaggle](https://www.kaggle.com/datasets/sid321axn/malicious-urls-dataset)\n",
        "\n",
        "This problem is solved using 4 different Models.\n",
        "1. Random Forest\n",
        "2. Gradient Boosted Trees\n",
        "3. Single Layer Neural Network\n",
        "4. Transfer learrning\n"
      ],
      "metadata": {
        "id": "mkqTOFrfjP-s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dbb7RUUs8Le"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Installs\n",
        "!pip install tld\n",
        "!pip install tensorflow tensorflow_decision_forests\n",
        "%load_ext tensorboard\n",
        "# !pip install tensorflow-addons\n",
        "# !pip install tensorflow-model-analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNlXmD_EGsoi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import requests\n",
        "import tarfile\n",
        "from os import path\n",
        "from datetime import datetime\n",
        "import shutil\n",
        "\n",
        "from IPython.display import Image\n",
        "\n",
        "# Data handling libraries\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# URL libs\n",
        "from tld import get_tld, is_tld\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "# Visualization\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Tensorflow\n",
        "import tensorflow as tf\n",
        "import tensorflow_decision_forests as tfdf\n",
        "import tensorflow_hub as hub\n",
        "import tensorboard\n",
        "\n",
        "# sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Directory Clean up"
      ],
      "metadata": {
        "id": "vbWDYdV0BtNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logs_dir = \"logs\"\n",
        "models_dir = \"trained_models\"\n",
        "model_imaages = \"model_imaages\"\n",
        "\n",
        "def clean_directories():\n",
        "  shutil.rmtree(logs_dir, ignore_errors=True)\n",
        "  shutil.rmtree(models_dir, ignore_errors=True)\n",
        "  shutil.rmtree(model_imaages, ignore_errors=True)\n",
        "clean_directories()  \n",
        "\n",
        "def create_directories():\n",
        "  os.mkdir(model_imaages)\n",
        "\n",
        "def get_logs_dir(model_name):\n",
        "  run_timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  return f\"{logs_dir}/{model_name}/{run_timestamp}\""
      ],
      "metadata": {
        "id": "CA2rW7lKABLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dowload Data File"
      ],
      "metadata": {
        "id": "H3Q4SW7DcgTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://drive.google.com/file/d/1OzJbRtgU-Z80iL7GodIH8-ymKq58g7Az/view?usp=share_link'\n",
        "url_download_path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]\n",
        "download_file = \"malicious_phish.csv\"\n",
        "if not path.exists(download_file):\n",
        "  r = requests.get(url_download_path, allow_redirects=True)\n",
        "  open(download_file, 'wb').write(r.content)\n",
        "df = pd.read_csv(download_file)\n",
        "# df = df.sample(n=100000, random_state=2)\n",
        "df.info()\n",
        "df.sample(n=10)"
      ],
      "metadata": {
        "id": "zR2oNSkIcWDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Trained Model"
      ],
      "metadata": {
        "id": "zyXuod8ecnCR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ehq_rr3pifxW"
      },
      "outputs": [],
      "source": [
        "MODEL_URL = \"https://tfhub.dev/google/Wiki-words-250-with-normalization/2?tf-hub-format=compressed\"\n",
        "TAR_FILE_PATH = 'Wiki-words-250-with-normalization.tar.gz'\n",
        "DIRPATH = 'Wiki-words-250-with-normalization'\n",
        "\n",
        "if not path.exists(TAR_FILE_PATH):\n",
        "  with requests.get(MODEL_URL, allow_redirects=True) as r:\n",
        "    with open(TAR_FILE_PATH, 'wb') as z:\n",
        "      z.write(r.content)\n",
        "\n",
        "if not path.exists(DIRPATH):\n",
        " with tarfile.open(TAR_FILE_PATH, 'r') as tar_ref:\n",
        "   tar_ref.extractall(f\"./{DIRPATH}\")\n",
        "\n",
        "hub_layer = hub.KerasLayer(\n",
        "    DIRPATH,\n",
        "    output_shape=[250],  # Outputs a tensor with shape [batch_size, 20].\n",
        "    input_shape=[],     # Expects a tensor of shape [batch_size] as input.\n",
        "    dtype=tf.string) \n",
        "print(hub_layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxRUouHAiwZE"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_M3nCUjpiQZ"
      },
      "outputs": [],
      "source": [
        "counts = df['type'].value_counts()\n",
        "sns.barplot(x=counts.index, y=counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUEqZwGMrR9A"
      },
      "source": [
        "# Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edBjyZAN7oOY"
      },
      "outputs": [],
      "source": [
        "df['url_len'] = df['url'].apply(lambda x: len(str(x))) \n",
        "df['count_www'] = df['url'].apply(lambda i: i.count('www'))\n",
        "df['count_at'] = df['url'].apply(lambda i: i.count('@'))\n",
        "df['count_dir'] = df['url'].apply(lambda url: urlparse(url).path.count('/'))\n",
        "df['count_https'] = df['url'].apply(lambda i : i.count('https'))\n",
        "df['count_http'] = df['url'].apply(lambda i : i.count('http'))\n",
        "df['hostname_length'] = df['url'].apply(lambda i: len(urlparse(i).netloc))\n",
        "\n",
        "# Character counts\n",
        "df['count_percent'] = df['url'].apply(lambda i: i.count('%'))\n",
        "df['count_hyphen'] = df['url'].apply(lambda i: i.count('-'))\n",
        "\n",
        "# Query parameters features\n",
        "df['count_question_mark'] = df['url'].apply(lambda i: i.count('?'))\n",
        "df['count_equals'] = df['url'].apply(lambda i: i.count('='))\n",
        "\n",
        "df.sample(n=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0XSTffcqF7f"
      },
      "outputs": [],
      "source": [
        "#Use of IP or not in domain\n",
        "def having_ip_address(url):\n",
        "    match = re.search(\n",
        "        '(([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.'\n",
        "        '([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\/)|'  # IPv4\n",
        "        '((0x[0-9a-fA-F]{1,2})\\\\.(0x[0-9a-fA-F]{1,2})\\\\.(0x[0-9a-fA-F]{1,2})\\\\.(0x[0-9a-fA-F]{1,2})\\\\/)' # IPv4 in hexadecimal\n",
        "        '(?:[a-fA-F0-9]{1,4}:){7}[a-fA-F0-9]{1,4}', url)  # Ipv6\n",
        "    if match:\n",
        "        # print match.group()\n",
        "        return 1\n",
        "    else:\n",
        "        # print 'No matching pattern found'\n",
        "        return 0\n",
        "df['use_of_ip'] = df['url'].apply(lambda i: having_ip_address(i))\n",
        "df['use_of_ip'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkphuF3c65d_"
      },
      "outputs": [],
      "source": [
        "#First Directory Length\n",
        "def fd_length(url):\n",
        "    urlpath= urlparse(url).path\n",
        "    try:\n",
        "        return len(urlpath.split('/')[1])\n",
        "    except:\n",
        "        return 0\n",
        "df['fd_length'] = df['url'].apply(lambda i: fd_length(i))   \n",
        "sns.countplot(x='fd_length', data=df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fzcsen8CzGWT"
      },
      "outputs": [],
      "source": [
        "# Hostname extraction\n",
        "def contains_scheme(url):\n",
        "    scheme = urlparse(url).scheme\n",
        "    scheme = scheme.strip()\n",
        "    if len(scheme) > 0:\n",
        "        # print(match.group())        \n",
        "        return 1\n",
        "    else:\n",
        "        # print 'No matching pattern found'\n",
        "        return 0\n",
        "\n",
        "df['contains_scheme'] = df['url'].apply(lambda i: contains_scheme(i))\n",
        "\n",
        "sns.countplot(x='contains_scheme', data=df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peXrAsI39S8_"
      },
      "outputs": [],
      "source": [
        "def get_tld_length(i):\n",
        "    try:\n",
        "        return len(get_tld(i,fail_silently=True))\n",
        "    except:\n",
        "        return -1\n",
        "df['tld_length'] = df['url'].apply(lambda i: get_tld_length(i))\n",
        "sns.countplot(x='tld_length', data=df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Vxgz9T4-tkG"
      },
      "outputs": [],
      "source": [
        "def digit_count(url):\n",
        "    digits = 0\n",
        "    for i in url:\n",
        "        if i.isnumeric():\n",
        "            digits = digits + 1\n",
        "    return digits\n",
        "df['count_digits']= df['url'].apply(lambda i: digit_count(i))\n",
        "sns.histplot(x='count_digits', data=df, bins=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmwVEVj6-uTi"
      },
      "outputs": [],
      "source": [
        "def letter_count(url):\n",
        "    letters = 0\n",
        "    for i in url:\n",
        "        if i.isalpha():\n",
        "            letters = letters + 1\n",
        "    return letters\n",
        "df['count_letters']= df['url'].apply(lambda i: letter_count(i))\n",
        "sns.histplot(x='count_letters', hue=\"type\", data=df, bins=30)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_words(url):\n",
        "  # print(url)\n",
        "  # print(url[0])\n",
        "  words = re.split(',|_|-|!|:|/|&|=|\\?|\\.', url)\n",
        "  words = list(filter(lambda word: len(word.strip()) >= 2, words))\n",
        "  words = [item.lower() for item in words]\n",
        "  return \" \".join(words)\n",
        "  \n",
        "df['url_words'] = df['url'].apply(lambda i: get_words(i))"
      ],
      "metadata": {
        "id": "9jV97FOshOUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud, STOPWORDS \n",
        "from PIL import Image\n",
        "\n",
        "comment_words = '' \n",
        "stopwords = set(STOPWORDS)\n",
        "pic = np.array(Image.open(requests.get('http://www.clker.com/cliparts/O/i/x/Y/q/P/yellow-house-hi.png',stream=True).raw))\n",
        "\n",
        "def display_words(words_column, type_name):\n",
        "  words = \"\"\n",
        "  df_column = words_column[words_column['type']==type_name]\n",
        "  url_words = \" \".join(df_column['url_words'].values)\n",
        "  # print(url_words)\n",
        "  wordcloud = WordCloud(width = 800, height = 800, \n",
        "                  background_color ='white', \n",
        "                  stopwords = stopwords, mask = pic, \n",
        "                  min_font_size = 10).generate(url_words)\n",
        "  plt.figure(figsize = (10, 10), facecolor = 'white', edgecolor='blue') \n",
        "  plt.imshow(wordcloud) \n",
        "  plt.axis(\"off\") \n",
        "  plt.tight_layout(pad = 0) \n",
        "    \n",
        "  plt.show()\n",
        "\n",
        "words_column = df[['url_words', 'type']] \n",
        "print('\\n.  BENIGN WordCloud')\n",
        "display_words(words_column, 'benign')\n",
        "print('\\n.  DEFACEMENT WordCloud')\n",
        "display_words(words_column, 'defacement')\n",
        "print('\\n.  PHISHING WordCloud')\n",
        "display_words(words_column, 'phishing')\n",
        "print('\\n MALWARE WordCloud')\n",
        "display_words(words_column, 'malware')\n"
      ],
      "metadata": {
        "id": "W2_b7W7obt3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObB94OzxrZeT"
      },
      "outputs": [],
      "source": [
        "df.sample(n=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDVJJSaBBj_W"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 15))\n",
        "sns.heatmap(df.corr(), linewidths=.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-A9JRJtAEzp"
      },
      "outputs": [],
      "source": [
        "class_names = [\"benign\",'defacement','phishing', 'malware']\n",
        "rem = {\"class\": {\"benign\": 0, \"defacement\": 1, \"phishing\":2, \"malware\":3}}\n",
        "df['class'] = df['type']\n",
        "df = df.replace(rem)\n",
        "# df['class'] = df['class'].apply(lambda i: tf.keras.utils.to_categorical(i, num_classes=4))\n",
        "# y = tf.keras.utils.to_categorical(df['class'], num_classes=4)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Test Split"
      ],
      "metadata": {
        "id": "IWmE8byBgr1G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTX9r-7eETgR"
      },
      "outputs": [],
      "source": [
        "train_size = int(0.8*len(df.index))\n",
        "val_size = int(0.1*len(df.index))\n",
        "\n",
        "train_split_df, test_split_df = train_test_split(\n",
        "    df, test_size=0.2, random_state=1, shuffle=True)\n",
        "# train_df, val_df, train_df = np.split(df, [train_size, train_size+val_size])\n",
        "\n",
        "print(len(train_split_df.index))\n",
        "# print(len(val_df.index))\n",
        "print(len(test_split_df.index))\n",
        "\n",
        "NN_EPOCHS = 20\n",
        "PATIENCE = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest Model"
      ],
      "metadata": {
        "id": "csVu9SpxgNcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_get_random_forest_model():\n",
        "  \n",
        "  train_df = train_split_df.drop(['url_words','url','type'],axis=1)\n",
        "  test_df = test_split_df.drop(['url_words','url','type'],axis=1)\n",
        "  \n",
        "  # Convert the dataset into a TensorFlow dataset.\n",
        "  train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_df, label=\"class\")\n",
        "  val_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_df, label=\"class\")\n",
        "  \n",
        "  model = tfdf.keras.RandomForestModel()\n",
        "  model_metrics = [\n",
        "      \"accuracy\"\n",
        "  ]\n",
        "  model.compile(metrics=model_metrics)\n",
        "\n",
        "  # Define the Keras TensorBoard callback.\n",
        "  logdir=get_logs_dir(\"random_forest\")\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "\n",
        "  model.fit(\n",
        "      train_ds, \n",
        "      validation_data=val_ds,\n",
        "      callbacks=[tensorboard_callback])\n",
        "  \n",
        "  return model\n"
      ],
      "metadata": {
        "id": "Tw4UQeDyf6Fw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Boosted Trees Model"
      ],
      "metadata": {
        "id": "50mhziFLgSkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_get_boosted_trees_model():\n",
        "\n",
        "  train_df = train_split_df.drop(['url_words','url','type'],axis=1)\n",
        "  test_df = test_split_df.drop(['url_words','url','type'],axis=1)\n",
        "  \n",
        "  # Convert the dataset into a TensorFlow dataset.\n",
        "  train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_df, label=\"class\")\n",
        "  val_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_df, label=\"class\")\n",
        "\n",
        "  model = tfdf.keras.GradientBoostedTreesModel()\n",
        "  model_metrics = [\n",
        "      \"accuracy\"\n",
        "  ]\n",
        "  model.compile(metrics=model_metrics)\n",
        "\n",
        "  # Define the Keras TensorBoard callback.\n",
        "  logdir=get_logs_dir(\"boosted_trees\")\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "\n",
        "  model.fit(\n",
        "      train_ds, \n",
        "      validation_data=val_ds,\n",
        "      callbacks=[tensorboard_callback])\n",
        "  return model"
      ],
      "metadata": {
        "id": "t2stn4fVgawk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network Model"
      ],
      "metadata": {
        "id": "s_YoNOeXOyDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "def save_and_display_model(model_instance):\n",
        "  run_timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  img_file = f'{model_imaages}/{run_timestamp}.png'\n",
        "  tf.keras.utils.plot_model(\n",
        "      model_instance, \n",
        "      to_file=img_file, \n",
        "      show_dtype=True,\n",
        "      show_shapes=True, \n",
        "      show_layer_names=True,\n",
        "      show_layer_activations=True,\n",
        "      dpi=100,\n",
        "      rankdir='LB'\n",
        "      )\n",
        "  display(Image(img_file))\n",
        "\n",
        "def train_and_get_neural_network_model():\n",
        "\n",
        "  # train_shuffled_df = train_split_df.sample(frac=1, random_state=2).reset_index()\n",
        "  # Convert the dataset into a TensorFlow dataset.\n",
        "  train_df = train_split_df.drop(['url','type'],axis=1)\n",
        "  test_df = test_split_df.drop(['url','type'],axis=1)\n",
        "  \n",
        "  # Convert the dataset into a TensorFlow dataset.\n",
        "  train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_df, label=\"class\")\n",
        "  val_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_df, label=\"class\")\n",
        "\n",
        "  # print(train_df.columns)\n",
        "  column_names = train_split_df.drop(labels = ['url_words', 'url','type', 'class'], axis=1)\n",
        "\n",
        "  def create_model():\n",
        "    real_valued_columns = [\n",
        "        tf.feature_column.numeric_column(key, shape=())\n",
        "        for key in column_names\n",
        "    ]\n",
        "    # print(real_valued_columns)\n",
        "    \n",
        "    input_layers = {\n",
        "        colname: tf.keras.layers.Input(name=colname, shape=(), dtype=tf.float32)\n",
        "        for colname in column_names\n",
        "    }\n",
        "    # print(input_layers)\n",
        "    dense_features_layer = tf.keras.layers.DenseFeatures(real_valued_columns)(input_layers)\n",
        "    # # Model\n",
        "    output = tf.keras.layers.Dense(4, activation='softmax')(dense_features_layer)\n",
        "    model_instance = tf.keras.Model(input_layers, output)\n",
        "\n",
        "    # Compile and fit\n",
        "    metrics = [\n",
        "        \"accuracy\"\n",
        "    ]\n",
        "    model_instance.compile(\n",
        "        optimizer=tf.keras.optimizers.SGD(),\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=metrics\n",
        "        )\n",
        "    \n",
        "    return model_instance\n",
        "  \n",
        "  model = create_model()\n",
        "  # Define the Keras TensorBoard callback.\n",
        "  logdir=get_logs_dir(\"neural_network_transfer_learning\")\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "  early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE)\n",
        "  weights_file = models_dir+\"/best_weights\"\n",
        "  model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=weights_file,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True)\n",
        "  \n",
        "  model.fit(\n",
        "      train_ds,\n",
        "      validation_data=val_ds,\n",
        "      epochs=NN_EPOCHS,\n",
        "      callbacks=[early_stopping_callback, model_checkpoint_callback, tensorboard_callback])\n",
        "  \n",
        "  best_model = create_model()\n",
        "  best_model.load_weights(weights_file)\n",
        "  return best_model"
      ],
      "metadata": {
        "id": "zZyaiQIhOyWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network with Transfer Learning Model"
      ],
      "metadata": {
        "id": "Z_cfKz98g4Wi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_get_neural_network_transfer_learning_model():\n",
        "\n",
        "  # train_shuffled_df = train_split_df.sample(frac=1, random_state=2).reset_index()\n",
        "  # Convert the dataset into a TensorFlow dataset.\n",
        "  train_df = train_split_df.drop(['url','type'],axis=1)\n",
        "  test_df = test_split_df.drop(['url','type'],axis=1)\n",
        "  \n",
        "  # Convert the dataset into a TensorFlow dataset.\n",
        "  train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_df, label=\"class\")\n",
        "  val_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_df, label=\"class\")\n",
        "\n",
        "  # print(train_df.columns)\n",
        "  column_names = train_split_df.drop(labels = ['url_words', 'url','type', 'class'], axis=1)\n",
        "\n",
        "  def create_model():\n",
        "    real_valued_columns = [\n",
        "        tf.feature_column.numeric_column(key, shape=())\n",
        "        for key in column_names\n",
        "    ]\n",
        "    # print(real_valued_columns)\n",
        "    \n",
        "    input_layers = {\n",
        "        colname: tf.keras.layers.Input(name=colname, shape=(), dtype=tf.float32)\n",
        "        for colname in column_names\n",
        "    }\n",
        "    # print(input_layers)\n",
        "    dense_features_layer = tf.keras.layers.DenseFeatures(real_valued_columns)(input_layers)\n",
        "\n",
        "    # Embeddings\n",
        "    url_words_input = tf.keras.Input(name='url_words', shape=tf.shape(''), dtype=tf.string)\n",
        "    input_layers.update({\n",
        "          \"url_words\": url_words_input   \n",
        "    })\n",
        "    # print(hub_layer)\n",
        "    embedding_layer = hub_layer(url_words_input)\n",
        "\n",
        "    # # Concatenation\n",
        "    # # input_layers['url_words'] = embedding_input_layer\n",
        "    all_feature_layers = tf.keras.layers.concatenate([dense_features_layer, embedding_layer])\n",
        "\n",
        "    # # Model\n",
        "    output = tf.keras.layers.Dense(4, activation='softmax')(all_feature_layers)\n",
        "    model_instance = tf.keras.Model(input_layers, output)\n",
        "\n",
        "    # Compile and fit\n",
        "    metrics = [\n",
        "        \"accuracy\",\n",
        "        # tf.keras.metrics.AUC(num_thresholds=3)\n",
        "    ]\n",
        "    model_instance.compile(\n",
        "        optimizer=tf.keras.optimizers.SGD(),\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=metrics\n",
        "        )\n",
        "    \n",
        "    return model_instance\n",
        "  \n",
        "  model = create_model()\n",
        "  # Define the Keras TensorBoard callback.\n",
        "  logdir=get_logs_dir(\"neural_network\")\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "  early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE)\n",
        "  weights_file = models_dir+\"/best_weights\"\n",
        "  model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=weights_file,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True)\n",
        "  \n",
        "  model.fit(\n",
        "      train_ds, \n",
        "      validation_data=val_ds,\n",
        "      epochs=NN_EPOCHS,\n",
        "      callbacks=[early_stopping_callback, model_checkpoint_callback, tensorboard_callback])\n",
        "  \n",
        "  best_model = create_model()\n",
        "  best_model.load_weights(weights_file)\n",
        "  return best_model\n"
      ],
      "metadata": {
        "id": "840kvjKsg4yB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training All Models"
      ],
      "metadata": {
        "id": "7TDfsSBmnDP2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_1Y1ozjH1nw"
      },
      "outputs": [],
      "source": [
        "clean_directories() \n",
        "create_directories()\n",
        "\n",
        "model_functions = [\n",
        "    train_and_get_random_forest_model, \n",
        "    train_and_get_boosted_trees_model,\n",
        "    train_and_get_neural_network_model,\n",
        "    train_and_get_neural_network_transfer_learning_model\n",
        "]\n",
        "\n",
        "trained_models = []\n",
        "for model_function in model_functions:\n",
        "  model = model_function()\n",
        "  trained_models.append(model)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkrB5bMFrsKl"
      },
      "outputs": [],
      "source": [
        "for model in trained_models:\n",
        "  test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_split_df, label=\"class\")\n",
        "  model_metrics = model.evaluate(test_ds)\n",
        "  results = model.predict(test_ds)\n",
        "  predictions = pd.DataFrame(results)\n",
        "  predictions['class'] = predictions.apply(lambda row : np.argmax(row), axis = 1)\n",
        "  cf_matrix = metrics.confusion_matrix(test_split_df[\"class\"], predictions['class'])\n",
        "\n",
        "  print(f\"\\n\\n Model: {model.__class__.__name__}\") \n",
        "  print(f\"\\n Accuracy: {model_metrics[1]}    Loss: {model_metrics[0]} \\n\")\n",
        "\n",
        "  print('\\n Confusion_matrix \\n')\n",
        "  plot_ = sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True,fmt= '0.2%')\n",
        "  plt.show()\n",
        "\n",
        "  print('\\n Classification_report \\n')\n",
        "  print(metrics.classification_report(test_split_df[\"class\"], predictions['class'], digits=3))\n",
        "  if model.__class__.__name__ != 'Functional':\n",
        "    display(tfdf.model_plotter.plot_model_in_colab(model, tree_idx=0, max_depth=3))\n",
        "  else:\n",
        "    save_and_display_model(model)  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs"
      ],
      "metadata": {
        "id": "vfnK4Yhm-nmg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}