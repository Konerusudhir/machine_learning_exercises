{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Konerusudhir/machine_learning_exercises/blob/master/SemanticSearch0_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bs4SOCFh_geZ"
      },
      "source": [
        "# **MP NFTs Semantic Search using Text and Images**\n",
        "\n",
        "I fetch MakersPlace NFTs(10K) and Index them using Faiss. Search is performed using Query embedding on the Text index or Image embedding on Image index. Below are the individual steps.\n",
        "\n",
        "1.   Fetch Makers Place Contract NFT Token ids(n ...m) meta data using Alchemy \n",
        "     API. Extract previwe image URLs from the response\n",
        "2.   Fetch Title text for each NFT. Parallell execution is used.\n",
        "3.   Download Clip Model(openai/clip-vit-large-patch14) from HuggingFace and instantiate TextModel and VIsion Model.\n",
        "4.   Create Title Text Index using Faiss\n",
        "5.   Generate text embeddings for Titles using Clip Text Model and add them to text index index\n",
        "6.   Load pre generated Image index\n",
        "7.   Build Gradio App for searching NFTs\n",
        "8.   Load Web App using Public URL and have fun\n",
        "\n",
        "## Acknowledgements\n",
        "1. Text index is built using NFT Titles only. Limited text to alpha numeric characters only. \n",
        "2. This is setup is not evaluated using curated/benchmark dataset to verify\n",
        "   accuracy\n",
        "3. Index generation and Index search is not on GPU. Only Image embedding generation is in GPU\n",
        "4. GPU Memory clean up is done manually. Need fine tuning to avoid OOM errors\n",
        "5. Search results in unintended images like NSFW images\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "g0YWIeS3WIw6"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install gradio\n",
        "!pip install transformers\n",
        "!pip install faiss-gpu\n",
        "!pip install torch\n",
        "!pip install Pillow\n",
        "!pip install matplotlib\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3TS_OgcrXbi",
        "outputId": "18724611-0ebf-4e4f-df81-18be39a52ba1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import math \n",
        "import glob\n",
        "import json\n",
        "import pickle\n",
        "import requests\n",
        "import time\n",
        "import re\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import spacy\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import numpy as np\n",
        "from numpy import random\n",
        "from PIL import Image\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import faiss\n",
        "\n",
        "DIMENSIONS = 768\n",
        "GATEWAY_URL = \"https://eth-mainnet.g.alchemy.com/nft/v2/rBshNbJGutTbf2ACdQ9XyGhhc1uSolds/getNFTMetadataBatch\"\n",
        "IPFS_GATEWAY = 'https://ipfsgateway.makersplace.com/ipfs/'\n",
        "MP_CONTRACT_ADDRESS = \"0x2963ba471e265e5f51cafafca78310fe87f8e6d1\"\n",
        "SUPPORTED_CONTENT_TYPES = ['image/jpeg','image/png','image/gif']\n",
        "IPFS_IMAGE_IDS_FILE_NAME = \"ipfs_image_ids.pickle\"\n",
        "INDEX_FOLDER = \"./indexes\"\n",
        "IMAGES_FOLDER = \"./images\"\n",
        "IPFS_IMAGE_IDS_PATH = os.path.join(INDEX_FOLDER, IPFS_IMAGE_IDS_FILE_NAME)\n",
        "MIN_TOKEN_ID = 1\n",
        "MAX_TOKEN_ID = 20000\n",
        "GATEWAY_QUERY_BATCH_SIZE = 100\n",
        "VISION_MODEL_INPUT_BATCH_SIZE = 8\n",
        "TEXT_MODEL_INPUT_BATCH_SIZE = 10\n",
        "SEARCH_RESULTS_DISPLAY_COUNT = 8\n",
        "LOG_DISPLAY_THRESHOLD = 1000\n",
        "RANDOM_SEED = 7\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# Load stop words\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gd2FLK9GNUam"
      },
      "outputs": [],
      "source": [
        "def clean_directories():\n",
        "  shutil.rmtree(IMAGES_FOLDER, ignore_errors=True)\n",
        "  shutil.rmtree(INDEX_FOLDER, ignore_errors=True)\n",
        "\n",
        "def create_directories():\n",
        "  if not os.path.exists(IMAGES_FOLDER):\n",
        "    os.mkdir(IMAGES_FOLDER) \n",
        "  if not os.path.exists(INDEX_FOLDER):\n",
        "    os.mkdir(INDEX_FOLDER)\n",
        "\n",
        "# clean_directories()\n",
        "create_directories()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fetch NFTs metadata"
      ],
      "metadata": {
        "id": "lSyQ-Q5_SAAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "class FetchClass:\n",
        "    def __init__(self):\n",
        "        self.image_duplicates_count = 0\n",
        "        self.text_descriptions = np.empty((0,3), str)\n",
        "        self.ipfs_image_ids = set()\n",
        "\n",
        "    def fetch_image_urls(self, min_token_id):\n",
        "        max_toke_id = min_token_id + GATEWAY_QUERY_BATCH_SIZE\n",
        "        token_id_requests = [] \n",
        "        for i in range(min_token_id, max_toke_id):\n",
        "            token_id_requests.append(\n",
        "                {\n",
        "                    \"contractAddress\": MP_CONTRACT_ADDRESS,\n",
        "                    \"tokenId\": f\"{i}\",\n",
        "                    \"tokenType\": \"ERC721\"\n",
        "                }\n",
        "            )\n",
        "\n",
        "        payload = {\n",
        "            \"tokens\": token_id_requests,\n",
        "            \"refreshCache\": False\n",
        "        }\n",
        "        headers = {\n",
        "            \"accept\": \"application/json\",\n",
        "            \"content-type\": \"application/json\"\n",
        "        }\n",
        "\n",
        "        responses = requests.post(GATEWAY_URL, json=payload, headers=headers)\n",
        "        nfts_metadata = json.loads(responses.text)\n",
        "\n",
        "        for nft_metadata in nfts_metadata:\n",
        "            imageUrl = nft_metadata['metadata'].get('imageUrl', None)\n",
        "            description = nft_metadata.get('title', None)\n",
        "            tokenId = nft_metadata['id'].get('tokenId', None)\n",
        "            if imageUrl is not None and description is not None:\n",
        "                ipfs_id = imageUrl.split('/')[-1]\n",
        "                if ipfs_id not in self.ipfs_image_ids:\n",
        "                    self.ipfs_image_ids.add(ipfs_id)\n",
        "                    self.text_descriptions = np.append(\n",
        "                        self.text_descriptions, \n",
        "                        np.array([[ipfs_id,description,tokenId]]), axis=0)\n",
        "                else:\n",
        "                    self.image_duplicates_count += 1\n",
        "                    if self.image_duplicates_count%LOG_DISPLAY_THRESHOLD == 0:\n",
        "                        print(f\"Found {self.image_duplicates_count} duplicates\")\n",
        "        \n",
        "np_file_prefix = \"mp_nft_data_np_array_12k.pickle\"\n",
        "np_file_name = f\"{np_file_prefix}.npy\"\n",
        "np_file_path = os.path.join(INDEX_FOLDER, np_file_name)\n",
        "\n",
        "if os.path.exists(np_file_path):\n",
        "    text_descriptions = np.load(np_file_path)\n",
        "else:\n",
        "    fetch_class = FetchClass()\n",
        "    for batch_start_index in range(\n",
        "        MIN_TOKEN_ID, MAX_TOKEN_ID, GATEWAY_QUERY_BATCH_SIZE):\n",
        "        \n",
        "        fetch_class.fetch_image_urls(batch_start_index)\n",
        "        \n",
        "        if (batch_start_index - 1)%LOG_DISPLAY_THRESHOLD == 0:\n",
        "            print(f\"Fetched batch from {batch_start_index}\")\n",
        "        \n",
        "        time.sleep(0.1)    \n",
        "\n",
        "    text_descriptions = fetch_class.text_descriptions\n",
        "    np.save(np_file_path, text_descriptions)\n",
        "\n",
        "print(f\"\"\"\n",
        "    Text Array shape: {text_descriptions.shape}\n",
        "    \"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41wWSOxzR_kj",
        "outputId": "15ccfcce-686e-4a4a-8c69-f4140d4e9969"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Text Array shape: (12306, 3)\n",
            "    \n",
            "CPU times: user 8.17 ms, sys: 70.9 ms, total: 79.1 ms\n",
            "Wall time: 79.6 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clean NFT title Text"
      ],
      "metadata": {
        "id": "apHyM1yATZL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_string(text, stem=\"None\"):\n",
        "\n",
        "    final_string = \"\"\n",
        "\n",
        "    text = re.sub(r\"[^a-zA-Z0-9 ]\", \"\", text)\n",
        "\n",
        "    # Make lower\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove line breaks\n",
        "    text = re.sub(r'\\n', '', text)\n",
        "\n",
        "    # Remove puncuation\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(translator)\n",
        "\n",
        "    # Remove stop words\n",
        "    text = text.split()\n",
        "    useless_words = nltk.corpus.stopwords.words(\"english\")\n",
        "    useless_words = useless_words + ['hi', 'im']\n",
        "\n",
        "    text_filtered = [word for word in text if not word in useless_words]\n",
        "\n",
        "    # Remove numbers\n",
        "    # text_filtered = [re.sub(r'\\w*\\d\\w*', '', w) for w in text_filtered]\n",
        "\n",
        "    # Stem or Lemmatize\n",
        "    if stem == 'Stem':\n",
        "        stemmer = PorterStemmer() \n",
        "        text_stemmed = [stemmer.stem(y) for y in text_filtered]\n",
        "    elif stem == 'Lem':\n",
        "        lem = WordNetLemmatizer()\n",
        "        text_stemmed = [lem.lemmatize(y) for y in text_filtered]\n",
        "    elif stem == 'Spacy':\n",
        "        text_filtered = nlp(' '.join(text_filtered))\n",
        "        text_stemmed = [y.lemma_ for y in text_filtered]\n",
        "    else:\n",
        "        text_stemmed = text_filtered\n",
        "    \n",
        "    partial_string = text_stemmed[0:60]\n",
        "    final_string = ' '.join(partial_string)\n",
        "\n",
        "    return final_string\n",
        "\n"
      ],
      "metadata": {
        "id": "QxbxWFnLTY1C"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hH5xob33CUAK"
      },
      "source": [
        "# 3 - Download Clip Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlcIFY3fUvhU",
        "outputId": "e54798c1-a4d4-42a5-c930-9b9b9f4a36d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModelWithProjection: ['vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'logit_scale', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'visual_projection.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.22.layer_norm2.bias']\n",
            "- This IS expected if you are initializing CLIPTextModelWithProjection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CLIPTextModelWithProjection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPVisionModelWithProjection: ['text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'logit_scale', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_projection.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.4.mlp.fc1.bias']\n",
            "- This IS expected if you are initializing CLIPVisionModelWithProjection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CLIPVisionModelWithProjection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 7.41 s, sys: 6.32 s, total: 13.7 s\n",
            "Wall time: 18.4 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from transformers import CLIPTokenizer, CLIPTextModelWithProjection, CLIPProcessor, CLIPVisionModelWithProjection, TFCLIPTextModel, TFCLIPVisionModel\n",
        "\n",
        "\n",
        "clip_model_id = \"openai/clip-vit-large-patch14\"\n",
        "\n",
        "text_model = CLIPTextModelWithProjection.from_pretrained(clip_model_id)\n",
        "tokenizer = CLIPTokenizer.from_pretrained(clip_model_id)\n",
        "\n",
        "vision_model = CLIPVisionModelWithProjection.from_pretrained(clip_model_id)\n",
        "processor = CLIPProcessor.from_pretrained(clip_model_id)\n",
        "\n",
        "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "# vision_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Title Index"
      ],
      "metadata": {
        "id": "UMmP_z79TeNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "TEXT_MODEL_INPUT_BATCH_SIZE = 200\n",
        "storage = \"Flat\"\n",
        "index_name = f\"IDMap,{storage}\"\n",
        "text_index = faiss.index_factory(DIMENSIONS, index_name)\n",
        "descriptions_ids_map = {}\n",
        "banned_token_ids = {\n",
        "    1185 # hash special charaters which is crashing Tokenizer\n",
        "}\n",
        "\n",
        "def get_text_embeds(queries):\n",
        "    inputs = tokenizer(queries, padding=True, return_tensors=\"pt\")\n",
        "    outputs = text_model(**inputs)\n",
        "    return outputs.text_embeds.detach().numpy()\n",
        "\n",
        "def build_text_index():\n",
        "  \n",
        "    text_batch = []\n",
        "    ipfs_ids = []\n",
        "    embeds_count = 0\n",
        "    batches = np.array_split(text_descriptions, TEXT_MODEL_INPUT_BATCH_SIZE)\n",
        "    \n",
        "    for text_descriptions_batch in batches:    \n",
        "        cleaned_text = []\n",
        "        ipfs_ids_hashes = []\n",
        "        title_text_batch = text_descriptions_batch[:,1].tolist()\n",
        "        batch_length = len(title_text_batch)\n",
        "\n",
        "        for i in range(batch_length):\n",
        "            token_id = int(text_descriptions_batch[i][2])\n",
        "            if token_id not in banned_token_ids:\n",
        "                ipfs_id = text_descriptions_batch[i][0]\n",
        "                ipfs_id_hash = hash(ipfs_id)\n",
        "                ipfs_ids_hashes.append(ipfs_id_hash)\n",
        "                descriptions_ids_map[ipfs_id_hash] = ipfs_id\n",
        "                cleaned_string = clean_string(title_text_batch[i])\n",
        "                cleaned_text.append(cleaned_string)\n",
        "            \n",
        "        if len(cleaned_text) > 0:\n",
        "            try:\n",
        "                text_description_embeds = get_text_embeds(cleaned_text)                            \n",
        "                text_index.add_with_ids(text_description_embeds, np.array(ipfs_ids_hashes))\n",
        "                embeds_count+=batch_length      \n",
        "                if embeds_count%2 == 0:\n",
        "                    print(f\"Created embeds for {embeds_count} descriptions\")\n",
        "            except Exception as e:        \n",
        "                print(f\"\"\"\n",
        "                Text : {text_descriptions_batch}  - cleaned: {cleaned_text}\n",
        "                IPFS Hashes: {ipfs_id_hash} \n",
        "                TokenIds:    {token_id}\n",
        "                \"\"\")\n",
        "\n",
        "\n",
        "text_index_file_prefix = index_name.replace(',', '_')\n",
        "text_index_file_name = f\"{text_index_file_prefix}_text.index\"\n",
        "text_index_file_path = os.path.join(INDEX_FOLDER, text_index_file_name)\n",
        "if os.path.exists(text_index_file_path):\n",
        "    text_index = faiss.read_index(text_index_file_path)\n",
        "else:    \n",
        "    build_text_index()\n",
        "    faiss.write_index(text_index, text_index_file_path)\n",
        "\n",
        "\n",
        "text_id_map_file_name = f\"{text_index_file_prefix}_text_ids.pickle\"\n",
        "text_id_map_path = os.path.join(INDEX_FOLDER, text_id_map_file_name)\n",
        "if os.path.exists(text_id_map_path):\n",
        "    with open(text_id_map_path, 'rb') as f:\n",
        "        descriptions_ids_map = pickle.load(f)\n",
        "else:\n",
        "    with open(text_id_map_path, 'wb') as f:\n",
        "        pickle.dump(descriptions_ids_map, f)\n",
        "\n",
        "\n",
        "print(f\"Text Index Size: {text_index.ntotal}\")\n",
        "print(f\"Text Ids Size: {len(descriptions_ids_map)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1sEEy2HTdg6",
        "outputId": "4a49ec34-dc21-423b-f0cc-4f8869c2f392"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Index Size: 12305\n",
            "Text Ids Size: 12305\n",
            "CPU times: user 55.9 ms, sys: 22.3 ms, total: 78.3 ms\n",
            "Wall time: 132 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keKwhGGCCjbb"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRgu_M7L7tck",
        "outputId": "f19e06cf-1de5-4781-f652-f0b0958256f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 10 µs, sys: 0 ns, total: 10 µs\n",
            "Wall time: 14.3 µs\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "def load_resized_image(img_path, max_width = 300, max_height = 300):\n",
        "    try:\n",
        "        if 'http' in img_path:\n",
        "            # response = requests.get(img_path, stream=True)\n",
        "            img = Image.open(requests.get(img_path, stream=True).raw)\n",
        "        else:    \n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "        width, height = img.size\n",
        "        if width > max_width or height > max_height:\n",
        "            img.thumbnail((max_width, max_height))\n",
        "        img = np.asarray(img)\n",
        "        return img    \n",
        "    except Exception as e:\n",
        "        print(f\"IPFS ID:{img_path.split('/')[-1]} - {e}\") \n",
        "        return None\n",
        "\n",
        "def download_images(ipfs_ids):\n",
        "\n",
        "    def fetch_image(ipfs_image_id):\n",
        "        image_url = os.path.join(IPFS_GATEWAY,ipfs_image_id) \n",
        "        image_local_path = os.path.join(IMAGES_FOLDER,ipfs_image_id)\n",
        "\n",
        "        # print(f\"{image_url} -- {file_name} -- {image_local_path}\")\n",
        "        \n",
        "        if not os.path.exists(image_local_path):    \n",
        "            response = requests.get(image_url)\n",
        "            content_type = response.headers.get('content-type')\n",
        "            if response.status_code and content_type in SUPPORTED_CONTENT_TYPES:\n",
        "                fp = open(image_local_path, 'wb')\n",
        "                fp.write(response.content)\n",
        "                fp.close()\n",
        "            else:\n",
        "                print(f\"HTTP Code:{response.status_code} - {content_type} - IPFS ID:{ipfs_image_id}\")\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=32) as executor:\n",
        "        executor.map(fetch_image, ipfs_ids)\n",
        "\n",
        "def search_descriptions_using_text(local_text_index, text_strings):\n",
        "    search_results = []\n",
        "    searh_embeds = get_text_embeds(text_strings)    \n",
        "    _, description_ids = local_text_index.search(searh_embeds, SEARCH_RESULTS_DISPLAY_COUNT)  \n",
        "    return description_ids \n",
        "\n",
        "\n",
        "def read_ipfs_image_ids(image_ids_path):\n",
        "    if os.path.exists(image_ids_path):\n",
        "        with open(image_ids_path, 'rb') as f:\n",
        "            return pickle.load(f)\n",
        "    else:\n",
        "        return set()\n",
        "\n",
        "def get_image_embeds(images):\n",
        "  inputs = processor(images=images, return_tensors=\"pt\")\n",
        "  outputs = vision_model(**inputs)\n",
        "  return outputs.image_embeds\n",
        "\n",
        "def search_images_using_images(image_index, search_images):\n",
        "    search_results = []\n",
        "    # for search_image in search_images:        \n",
        "    try:\n",
        "        searh_embeds = get_image_embeds(search_images).detach().numpy()        \n",
        "        _, image_ids = image_index.search(searh_embeds, SEARCH_RESULTS_DISPLAY_COUNT)\n",
        "        search_results.extend(image_ids[0])\n",
        "    except Exception as e:\n",
        "        print(f\"Bad Image IPFS ID:{search_images} - {e}\") \n",
        "    return search_results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradio Image search App"
      ],
      "metadata": {
        "id": "OTNo-Cf7fE3g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GR6iR6BKh1Ka",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 695
        },
        "outputId": "2db63473-ca8d-4e9d-d19e-6d9eb6261a03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Index size: 12305\n",
            "Image Index size: 10957\n",
            "Text Ids size: 12305\n",
            "Image Ids size: 10957\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://7428be80-d0ec-47f4.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://7428be80-d0ec-47f4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://7428be80-d0ec-47f4.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Download some images from ipfs ids to boot strap image search App\n",
        "loaded_ipfs_image_ids = list(read_ipfs_image_ids(IPFS_IMAGE_IDS_PATH))\n",
        "# download_images(loaded_ipfs_image_ids[:30])\n",
        "\n",
        "# Load Image index\n",
        "image_index_prefix = index_name.replace(',', '_')\n",
        "image_index_file_name = f\"{image_index_prefix}.index\"\n",
        "index_path = os.path.join(INDEX_FOLDER, image_index_file_name)\n",
        "loaded_image_index = faiss.read_index(index_path)\n",
        "# Load Image Id map\n",
        "id_map_file_name = f\"{image_index_prefix}_image_ids.pickle\"\n",
        "id_map_path = os.path.join(INDEX_FOLDER, id_map_file_name)\n",
        "with open(id_map_path, 'rb') as f:\n",
        "    loaded_image_ids_map = pickle.load(f)\n",
        "\n",
        "print(f\"Text Index size: {text_index.ntotal}\")\n",
        "print(f\"Image Index size: {loaded_image_index.ntotal}\")\n",
        "print(f\"Text Ids size: {len(descriptions_ids_map)}\")\n",
        "print(f\"Image Ids size: {len(loaded_image_ids_map)}\")\n",
        "\n",
        "def search_images_by_image(query_image):\n",
        "    return search_images('', query_image)\n",
        "\n",
        "def search_images(query_text, query_image):\n",
        "    images_to_download = []\n",
        "    # print(f\"Received {query_text}:{query_image}\")\n",
        "    if len(query_text) > 0:\n",
        "        ids_map = descriptions_ids_map\n",
        "        search_results = search_descriptions_using_text(text_index, [query_text]).flatten()\n",
        "        for image_hash in search_results:\n",
        "            images_to_download.append(ids_map[image_hash])\n",
        "        \n",
        "    elif query_image is not None:\n",
        "        ids_map = loaded_image_ids_map\n",
        "        search_results = search_images_using_images(loaded_image_index, [query_image])\n",
        "        # print(f\"Search results: {search_results}\")\n",
        "        for image_hash in search_results:\n",
        "            ipfs_id = ids_map.get(image_hash, None)\n",
        "            # if ipfs_id is not None:\n",
        "            images_to_download.append(ids_map[image_hash])\n",
        "    # print(f\"Images to Download: {images_to_download}\")        \n",
        "    download_images(images_to_download)\n",
        "    # print(f\"search_results: {search_results}\")\n",
        "    # print(f\"Images to download: {images_to_download}\")\n",
        "    image_objects = []\n",
        "    for image_hash in search_results:\n",
        "        image_id = ids_map[image_hash]\n",
        "        local_path = os.path.join(IMAGES_FOLDER, image_id)\n",
        "        image_objects.append(load_resized_image(local_path))\n",
        "    return image_objects\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Column(scale=1):\n",
        "        query_text = gr.Textbox(label=\"query_text\", value=\"universe\")\n",
        "        query_image = gr.Image(label=\"query_image\")\n",
        "        search_btn = gr.Button(\"Search\")        \n",
        "    \n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            result_0_image = gr.Image(type=\"pil\", label=\"result_0_image\")\n",
        "            result_0_html =  gr.HTML(label=\"result_0_html\", show_label = True)\n",
        "            result_0_btn = gr.Button(\"Related Images\")    \n",
        "        with gr.Column(scale=1):\n",
        "            result_1_image = gr.Image(type=\"pil\", label=\"result_1_image\")\n",
        "            result_1_btn = gr.Button(\"Related Images\")  \n",
        "        with gr.Column(scale=1):\n",
        "            result_2_image = gr.Image(type=\"pil\", label=\"result_2_image\")\n",
        "            result_2_btn = gr.Button(\"Related Images\")  \n",
        "    with gr.Row():    \n",
        "        with gr.Column(scale=1):\n",
        "            result_3_image = gr.Image(type=\"pil\", label=\"result_3_image\")\n",
        "            result_3_btn = gr.Button(\"Related Images\")  \n",
        "        with gr.Column(scale=1):\n",
        "            result_4_image = gr.Image(type=\"pil\", label=\"result_4_image\")\n",
        "            result_4_btn = gr.Button(\"Related Images\")  \n",
        "        with gr.Column(scale=1):\n",
        "            result_5_image = gr.Image(type=\"pil\", label=\"result_5_image\")\n",
        "            result_5_btn = gr.Button(\"Related Images\")  \n",
        "\n",
        "    inputs = [query_text, query_image]    \n",
        "    outputs = [\n",
        "        result_0_image, \n",
        "        result_1_image, \n",
        "        result_2_image, \n",
        "        result_3_image, \n",
        "        result_4_image,\n",
        "        result_5_image]\n",
        "\n",
        "    search_btn.click(fn=search_images, inputs=inputs, outputs=outputs)\n",
        "    \n",
        "    result_0_btn.click(fn=search_images_by_image, \n",
        "                       inputs=result_0_image, outputs=outputs)\n",
        "    result_1_btn.click(fn=search_images_by_image, \n",
        "                       inputs=result_1_image, outputs=outputs)\n",
        "    result_2_btn.click(fn=search_images_by_image, \n",
        "                       inputs=result_2_image, outputs=outputs)\n",
        "    result_3_btn.click(fn=search_images_by_image, \n",
        "                       inputs=result_3_image, outputs=outputs)\n",
        "    result_4_btn.click(fn=search_images_by_image, \n",
        "                       inputs=result_4_image, outputs=outputs)\n",
        "    result_5_btn.click(fn=search_images_by_image, \n",
        "                       inputs=result_5_image, outputs=outputs)\n",
        "    \n",
        "\n",
        "demo.launch(share=True, debug=True,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7GtRe6PbkUQ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c47b2804-90e3-45c0-e043-134434f4490f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing server running on port: 7860\n"
          ]
        }
      ],
      "source": [
        "demo.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "igRQ8S7HSs2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e6b179df-7a64-4f83-d9c7-e680d4018b74"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/indexes.tar'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# file_name = \"mp_nft_data_np_array_12k.pickle\"\n",
        "# file_path = os.path.join(INDEX_FOLDER, file_name)\n",
        "# np.save(file_path, fetch_class.text_descriptions)\n",
        "\n",
        "shutil.make_archive(f\"{INDEX_FOLDER}\", 'tar', INDEX_FOLDER)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAv5EcANfXN8"
      },
      "outputs": [],
      "source": [
        "# gpu_info = !nvidia-smi\n",
        "# gpu_info = '\\n'.join(gpu_info)\n",
        "# if gpu_info.find('failed') >= 0:\n",
        "#   print('Not connected to a GPU')\n",
        "# else:\n",
        "#   print(gpu_info)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}